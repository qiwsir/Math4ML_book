# 交叉熵损失函数

*打开本页，如果没有显示公式，请刷新页面。*

此内容为《机器学习数学基础》**7.4 相对熵和交叉熵** 补充资料。

----

在研究机器学习或深度学习问题时，损失函数或者代价函数——关于两者的区别，请参阅《机器学习数学基础》中的详细说明——是必不可少的，它们主要用以优化训练模型。目标就是让损失函数最小化，损失越小的模型越好。交叉熵损失函数，就是众多损失函数中重要一员，它主要用与对分类模型的优化。为了理解交叉熵损失函数，以及为什么同时用Softmax作为激活函数，特别撰写本文。

下面我们使用一个图像分类的示例，这个示例中包括狗、猫、马和豹。

![](https://gitee.com/qiwsir/images/raw/master/2021-2-13/1613173338441-image1.png)

如上图所示，以Softmax函数作为激活函数，交叉熵损失函数旨在度量预测值（$$P$$）与真实值之间的差距，如下图所示。

![](https://gitee.com/qiwsir/images/raw/master/2021-2-13/1613173585431-image2.png)

例如，如果输入图片是狗，其真实值为 $$[1,0,0,0]$$ ，但通过深度学习模型，得到的预测值为 $$[0.775, 0.116, 0.039, 0.070]$$ 。我们的目标就是要让输出的预测值与真实值之间尽可能地靠近。在模型训练过程中，将模型权重进行迭代调整，以最大程度地减少交叉熵损失。 权重的调整过程就是模型训练过程，并且随着模型的不断训练和损失的最小化，这就是机器学习中所说的学习过程。

交叉熵的概念起源于信息论，香农（[Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon)）在1948年创立了信息论，其中最重要的概念就是信息熵，所以，在学习交叉熵之前，要先了解信息熵——不过，下面仅仅是列出信息熵的基本概念，因为在《机器学习数学基础》一书中，有专门章节讨论信息熵的有关知识。

### 熵

随机变量 $$X$$ 的熵定义：

$$H(X)=\begin{cases}-\sum_xp(x)\log(p(x)),\quad &X是离散型随机变量\\-\int_xp(x)\log(p(x)),&X是连续型随机变量\end{cases}$$

关于熵的更多内容，请参阅《机器学习数学基础》（2021年5月，电子工业出版社出版）。

### 交叉熵损失函数

交叉熵损失函数，也称为对数损失或者logistic损失。当模型产生了预测值之后，将对类别的预测概率与真实值（由 $$0$$ 或 $$1$$ 组成）进行不比较，计算所产生的损失，然后基于此损失设置对数形式的惩罚项。

在训练模型的时候，使用交缠上损失函数，目的是最小化损失，即损失越小的模型越好。最理想的就是交叉熵损失函数为 $$0$$ 。

> **定义**
>
> $$L_{CE}=-\sum_{i=1}^nt_i\log(p_i)$$
>
> 其中，$$n$$ 是类别的数量，$$t_i$$ 是某个类别的真实值，$$p_i$$ 是该了别的预测概率。

一般情况下，取以 $$2$$ 为底的对数进行计算。

### 二分类交叉熵损失函数

对于二分类问题，由于分类结果服从伯努利分布（参阅《机器学习数学基础》），所以二分类交叉熵损失函数定义为：

> **定义**
>
> $$L=-\sum_{i=1}^nt_i\log(p_i)=-[t\log(p)+(1-t)\log(1-p)]$$
>
> 其中，$$t_i$$ 是某类别的真实值，取值为 $$0$$ 或 $$1$$ ；$$p_i$$ 为某类别的预测概率。

在二分类问题中，通常计算所有样本的平均交叉熵损失：

$$L=-\frac{1}{N}\left[\sum_{j=1}^Nt_j\log(p_j)+(1-t_j)\log(1-p_j)\right]$$

其中，$$N$$ 为样本数量，$$t_j$$ 为第 $$j$$ 个样本的真实类别值，$$p_j$$ 为相应样本的预测概率。

以前面提到的图片识别为例，$$S$$ 表示预测结果，$$T$$ 表示真实标签，如下图所示。

![](https://gitee.com/qiwsir/images/raw/master/2021-2-13/1613173585431-image2.png)

根据上面的数据，计算两者之间的交叉熵：

$$\begin{split}L&=-\sum_{i=1}^4T_i\log(S_i)\\&=-[1\log_2(0.775)+0\log_2(0.116)+0\log_2(0.039)+0\log_2(0.070)]\\&=-\log_2(0.775)\\&=0.3677\end{split}$$

在神经网络中，所使用的Softmax函数是连续可导函数，这使得可以计算出损失函数相对于神经网络中每个权重的导数（在《机器学习数学基础》中有对此的完整推导过程和案例，读者可以理解其深层含义，请参阅）。这样就可以相应地调整模型的权重以最小化损失函数（模型输出接近真实值）。

假设经过权重调整之后，其输出值变为：

![](https://gitee.com/qiwsir/images/raw/master/2021-2-13/1613176751441-image3.png)

用上面方法，可以容易计算出，这次交叉熵损失比原来小了。

在[Keras](https://keras.io/zh/)（一种高级神经网络接口，Google的TensorFlow在其核心库中已经支持Keras[2]）中提供了多种交叉熵损失函数：

- 二分类
- 多分类
- 稀疏类别

关于交叉熵损失函数的更多内容，建议参阅《机器学习数学基础》中的详细说明，本书于2021年5月由电子工业出版社出版。

## 参考文献

[1]. 齐伟.机器学习数学基础.北京：电子工业出版社，2021

[2]. https://zh.wikipedia.org/wiki/Keras

[3]. https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e